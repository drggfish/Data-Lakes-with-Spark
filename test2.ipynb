{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import (\n",
    "    year, \n",
    "    month, \n",
    "    dayofmonth, \n",
    "    hour, \n",
    "    weekofyear, \n",
    "    dayofweek, \n",
    "    date_format, \n",
    "    monotonically_increasing_id\n",
    ")\n",
    "from pyspark.sql.types import (\n",
    "    StructType, \n",
    "    StructField, \n",
    "    StringType, \n",
    "    IntegerType, \n",
    "    DoubleType, \n",
    "    TimestampType\n",
    ")\n",
    "from schema import SONG_DATA_SCHEMA, LOG_DATA_SCHEMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_song_data(spark, input_data, output_data):\n",
    "    # get filepath to song data file\n",
    "    song_data = os.path.join(input_data, \"song-data\", \"*\", \"*\", \"*\", \"*.json\")\n",
    "    \n",
    "    # read song data file\n",
    "    df = spark.read.json(song_data, schema=SONG_DATA_SCHEMA)\n",
    "\n",
    "    # extract columns to create songs table\n",
    "    songs_table = df.select('song_id', 'title', 'artist_id', 'year', 'artist_name', 'duration').distinct()\n",
    "    \n",
    "    # write songs table to parquet files partitioned by year and artist\n",
    "    songs_table.write.mode('overwrite').partitionBy(\n",
    "        \"year\", \"artist_id\").parquet(output_data + \"songs_table.parquet\")\n",
    "\n",
    "    # extract columns to create artists table\n",
    "    artists_table = df.select(\n",
    "    \"artist_id\",\n",
    "    col(\"artist_name\").alias(\"name\"),\n",
    "    col(\"artist_location\").alias(\"location\"),\n",
    "    col(\"artist_latitude\").alias(\"latitude\"),\n",
    "    col(\"artist_longitude\").alias(\"longitude\")\n",
    "    ).drop_duplicates()\n",
    "    \n",
    "    # write artists table to parquet files\n",
    "    artists_table.write.mode('overwrite').parquet(output_data + \"artist_table.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_log_data(spark, input_data, output_data):\n",
    "    # get filepath to log data file\n",
    "    # log_data = os.path.join(input_data, \"log-data\", \"*.json\") LOCAL PATH ON MY MACHINE\n",
    "    log_data = os.path.join(input_data, \"log-data\", \"*\", \"*\", \"*.json\")\n",
    "\n",
    "    # read log data file\n",
    "    df = spark.read.json(log_data, schema=LOG_DATA_SCHEMA)\n",
    "    \n",
    "    # filter by actions for song plays\n",
    "    df = df.filter(df.page == 'NextSong')\n",
    "\n",
    "    # create timestamp column from original timestamp column\n",
    "    get_timestamp = udf(lambda ts: datetime.utcfromtimestamp(ts / 1000.0).\n",
    "        strftime(\"%Y-%m-%d %H:%M:%S\"), \n",
    "        StringType()\n",
    "         )\n",
    "    df = df.withColumn('timestamp', get_timestamp(\"ts\"))\n",
    "    \n",
    "    # create datetime column from original timestamp column\n",
    "    get_datetime = udf(\n",
    "        lambda x: datetime.utcfromtimestamp(x / 1000),\n",
    "        TimestampType()\n",
    "    )\n",
    "    df = df.withColumn(\"start_time\", get_datetime(\"ts\"))\n",
    "    \n",
    "    # extract columns to create time table\n",
    "    time_table = (\n",
    "        df\n",
    "        .withColumn(\"hour\", hour(\"start_time\"))\n",
    "        .withColumn(\"day\", dayofmonth(\"start_time\"))\n",
    "        .withColumn(\"week\", weekofyear(\"start_time\"))\n",
    "        .withColumn(\"month\", month(\"start_time\"))\n",
    "        .withColumn(\"year\", year(\"start_time\"))\n",
    "        .withColumn(\"weekday\", dayofweek(\"start_time\"))\n",
    "        .select(\n",
    "            \"start_time\",\n",
    "            \"hour\",\n",
    "            \"day\",\n",
    "            \"week\",\n",
    "            \"month\",\n",
    "            \"year\",\n",
    "            \"weekday\"\n",
    "        )\n",
    "        .drop_duplicates([\"year\", \"month\", \"day\", \"hour\"])\n",
    "    )\n",
    "    \n",
    "    # write time table to parquet files partitioned by year and month\n",
    "    time_table.write.parquet(\n",
    "        os.path.join(output_data, \"time_table.parquet\"),\n",
    "        mode=\"overwrite\",\n",
    "        partitionBy=[\"year\", \"month\"]\n",
    "    )\n",
    "\n",
    "    # read in song data to use for songplays table\n",
    "    songs = spark.read.parquet(\n",
    "        os.path.join(output_data, \"songs_table.parquet\")\n",
    "    )\n",
    "\n",
    "    # extract columns from joined song and log datasets to create songplays table \n",
    "    songplays_table = (\n",
    "        df\n",
    "        .join(songs, [\n",
    "            df.song == songs.title,\n",
    "            df.artist == songs.artist_name,\n",
    "            df.length == songs.duration\n",
    "        ], \"left\")\n",
    "    )\n",
    "\n",
    "    songplays_table = (\n",
    "        songplays_table\n",
    "        .select(\n",
    "            monotonically_increasing_id().alias(\"songplay_id\"),\n",
    "            \"start_time\",\n",
    "            col(\"userID\").alias(\"user_id\"),\n",
    "            \"level\",\n",
    "            \"song_id\",\n",
    "            \"artist_id\",\n",
    "            col(\"sessionId\").alias(\"session_id\"),\n",
    "            \"location\",\n",
    "            col(\"userAgent\").alias(\"user_agent\"),\n",
    "            month(\"start_time\").alias(\"month\"),\n",
    "            year(\"start_time\").alias(\"year\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # write songplays table to parquet files partitioned by year and month\n",
    "    songplays_table.write.parquet(\n",
    "        os.path.join(output_data, \"songplays_table.parquet\"),\n",
    "        mode=\"overwrite\",\n",
    "        partitionBy=[\"year\", \"month\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    spark = create_spark_session()\n",
    "\n",
    "    input_data = 'data/'\n",
    "    output_data = 'data/output_data/'\n",
    "    \n",
    "    #input_data = \"s3a://udacity-dend/\"\n",
    "    #output_data = \"s3a://drggfish-spark-udacity-dend/\"\n",
    "    \n",
    "    process_song_data(spark, input_data, output_data)    \n",
    "    process_log_data(spark, input_data, output_data)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d344b0b44ed74f5219ec8dab31a9f3e764b5e17a5cf2d9830ceefda51f994047"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('tensorflow': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
